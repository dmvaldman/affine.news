name: Data Pipeline

on:
  schedule:
    - cron: '0 13,21,5 * * *' # Every 8 hours, starting at 6am PT (UTC-7)
  workflow_dispatch:
    inputs:
      max_articles:
        description: 'Max articles per paper'
        required: false
        default: ''

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install -r crawler/requirements.crawler.txt
      - name: Set up env
        env:
          GH_MAX: ${{ github.event.inputs.max_articles }}
        run: |
          if [ -z "$GH_MAX" ]; then echo "MAX_ARTICLES=30" >> $GITHUB_ENV; else echo "MAX_ARTICLES=$GH_MAX" >> $GITHUB_ENV; fi
      - name: Run crawl
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          python -m crawler.scripts.run_crawl_heuristic --max-articles "$MAX_ARTICLES"

  translate:
    runs-on: ubuntu-latest
    needs: crawl
    timeout-minutes: 60
    permissions:
      contents: 'read'
      id-token: 'write'
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Authenticate to Google Cloud
        uses: 'google-github-actions/auth@v2'
        with:
          credentials_json: '${{ secrets.GCP_SA_KEY }}'
      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install -r crawler/requirements.crawler.txt
      - name: Run translation script
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          GOOGLE_PROJECT_ID: ${{ secrets.GOOGLE_PROJECT_ID }}
          TRANSLATE_PROVIDER: "google"
        run: |
          python -m crawler.scripts.run_translate

  embed:
    runs-on: ubuntu-latest
    needs: translate
    timeout-minutes: 30
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install -r crawler/requirements.crawler.txt
      - name: Run embed script
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          python -m crawler.scripts.run_embed

  generate_topics:
    runs-on: ubuntu-latest
    needs: embed
    timeout-minutes: 20
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install -r scripts/requirements.bertopic.txt
      - name: Run generate topics script
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          BLOB_READ_WRITE_TOKEN: ${{ secrets.BLOB_READ_WRITE_TOKEN }}
        run: |
          python -m crawler.scripts.run_generate_topics

  spectrum_cache:
    runs-on: ubuntu-latest
    needs: generate_topics
    timeout-minutes: 60
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install -r crawler/requirements.crawler.txt
          pip install requests
      - name: Run spectrum cache script
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          python -m crawler.scripts.run_spectrum_cache


